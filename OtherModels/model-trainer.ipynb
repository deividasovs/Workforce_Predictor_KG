{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Regression NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from TestTrainData import TrainData, TestData\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model imports\n",
    "from MultiLabelRegression import MultiLabelRegression\n",
    "from MultiLayerMultiLabelRegression import MultiLayerMultiLabelRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules from parent directory\n",
    "sys.path.insert(0,'..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.training_preprocessing import GetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = GetDataset()\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set x equal to all columns except for the transaction_count and workforce_type_1\n",
    "x = df.drop(['transaction_count', 'workforce_type_1','workforce_type_2', 'workforce_type_3','workforce_type_4'], axis=1)\n",
    "# convert x to a 2d array\n",
    "x = x.values.tolist()\n",
    "\n",
    "\n",
    "# set y equal to the transaction_count and workforce_type_1 columns\n",
    "y = df[['transaction_count', 'workforce_type_1', 'workforce_type_2', 'workforce_type_3','workforce_type_4']]\n",
    "# convert y to an array\n",
    "y = y.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.33\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kg/9csh_qlx26z9c976g218wbz80000gn/T/ipykernel_8044/811930537.py:1: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  train_data = TrainData(torch.IntTensor(X_train), torch.IntTensor(y_train))\n",
      "/var/folders/kg/9csh_qlx26z9c976g218wbz80000gn/T/ipykernel_8044/811930537.py:3: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  test_data = TestData(torch.IntTensor(X_test))\n"
     ]
    }
   ],
   "source": [
    "train_data = TrainData(torch.IntTensor(X_train), torch.IntTensor(y_train))\n",
    "\n",
    "test_data = TestData(torch.IntTensor(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train(model, optimizer, criterion, num_epochs, train_loader):\n",
    "def train(model, scheduler, criterion, num_epochs, train_loader):\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward pass and optimization\n",
    "            #optimizer.zero_grad()\n",
    "            # If loss is chattering, reduce learning rate TODO\n",
    "            scheduler.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            scheduler.optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        print(scheduler.optimizer.param_groups[0]['lr'])\n",
    "            #optimizer.step()\n",
    "\n",
    "            \n",
    "        # Print progress\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelRegression(\n",
       "  (linear): Linear(in_features=11, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Models\n",
    "#model = MultiLayerMultiLabelRegression(11, 5)\n",
    "model = MultiLabelRegression(11, 5)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and the optimizer\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) # step size = how many epochs to update lr after, gamma = how much to update lr by\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "Epoch [1/1000], Loss: 28.4051\n",
      "0.001\n",
      "Epoch [2/1000], Loss: 16.2821\n",
      "0.001\n",
      "Epoch [3/1000], Loss: 12.6094\n",
      "0.001\n",
      "Epoch [4/1000], Loss: 4.0243\n",
      "0.001\n",
      "Epoch [5/1000], Loss: 2.5313\n",
      "0.001\n",
      "Epoch [6/1000], Loss: 2.2027\n",
      "0.001\n",
      "Epoch [7/1000], Loss: 2.2848\n",
      "0.001\n",
      "Epoch [8/1000], Loss: 2.3536\n",
      "0.001\n",
      "Epoch [9/1000], Loss: 2.4564\n",
      "0.001\n",
      "Epoch [10/1000], Loss: 2.5729\n",
      "0.001\n",
      "Epoch [11/1000], Loss: 2.7121\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "0.0001\n",
      "Epoch [12/1000], Loss: 2.8196\n",
      "0.0001\n",
      "Epoch [13/1000], Loss: 3.0449\n",
      "0.0001\n",
      "Epoch [14/1000], Loss: 3.1839\n",
      "0.0001\n",
      "Epoch [15/1000], Loss: 3.1347\n",
      "0.0001\n",
      "Epoch [16/1000], Loss: 3.0759\n",
      "0.0001\n",
      "Epoch [17/1000], Loss: 3.0193\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.\n",
      "1e-05\n",
      "Epoch [18/1000], Loss: 2.9670\n",
      "1e-05\n",
      "Epoch [19/1000], Loss: 2.0920\n",
      "1e-05\n",
      "Epoch [20/1000], Loss: 1.8328\n",
      "1e-05\n",
      "Epoch [21/1000], Loss: 1.7730\n",
      "1e-05\n",
      "Epoch [22/1000], Loss: 1.7715\n",
      "1e-05\n",
      "Epoch [23/1000], Loss: 1.7833\n",
      "1e-05\n",
      "Epoch [24/1000], Loss: 1.7949\n",
      "1e-05\n",
      "Epoch [25/1000], Loss: 1.8030\n",
      "1e-05\n",
      "Epoch [26/1000], Loss: 1.8074\n",
      "1e-05\n",
      "Epoch [27/1000], Loss: 1.8090\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-06.\n",
      "1.0000000000000002e-06\n",
      "Epoch [28/1000], Loss: 1.8087\n",
      "1.0000000000000002e-06\n",
      "Epoch [29/1000], Loss: 1.4710\n",
      "1.0000000000000002e-06\n",
      "Epoch [30/1000], Loss: 1.3994\n",
      "1.0000000000000002e-06\n",
      "Epoch [31/1000], Loss: 1.3785\n",
      "1.0000000000000002e-06\n",
      "Epoch [32/1000], Loss: 1.3708\n",
      "1.0000000000000002e-06\n",
      "Epoch [33/1000], Loss: 1.3667\n",
      "1.0000000000000002e-06\n",
      "Epoch [34/1000], Loss: 1.3639\n",
      "1.0000000000000002e-06\n",
      "Epoch [35/1000], Loss: 1.3614\n",
      "1.0000000000000002e-06\n",
      "Epoch [36/1000], Loss: 1.3593\n",
      "1.0000000000000002e-06\n",
      "Epoch [37/1000], Loss: 1.3574\n",
      "1.0000000000000002e-06\n",
      "Epoch [38/1000], Loss: 1.3557\n",
      "1.0000000000000002e-06\n",
      "Epoch [39/1000], Loss: 1.3542\n",
      "1.0000000000000002e-06\n",
      "Epoch [40/1000], Loss: 1.3529\n",
      "1.0000000000000002e-06\n",
      "Epoch [41/1000], Loss: 1.3516\n",
      "1.0000000000000002e-06\n",
      "Epoch [42/1000], Loss: 1.3506\n",
      "1.0000000000000002e-06\n",
      "Epoch [43/1000], Loss: 1.3496\n",
      "1.0000000000000002e-06\n",
      "Epoch [44/1000], Loss: 1.3488\n",
      "1.0000000000000002e-06\n",
      "Epoch [45/1000], Loss: 1.3481\n",
      "1.0000000000000002e-06\n",
      "Epoch [46/1000], Loss: 1.3475\n",
      "1.0000000000000002e-06\n",
      "Epoch [47/1000], Loss: 1.3469\n",
      "1.0000000000000002e-06\n",
      "Epoch [48/1000], Loss: 1.3463\n",
      "1.0000000000000002e-06\n",
      "Epoch [49/1000], Loss: 1.3458\n",
      "1.0000000000000002e-06\n",
      "Epoch [50/1000], Loss: 1.3454\n",
      "1.0000000000000002e-06\n",
      "Epoch [51/1000], Loss: 1.3450\n",
      "1.0000000000000002e-06\n",
      "Epoch [52/1000], Loss: 1.3446\n",
      "1.0000000000000002e-06\n",
      "Epoch [53/1000], Loss: 1.3443\n",
      "1.0000000000000002e-06\n",
      "Epoch [54/1000], Loss: 1.3440\n",
      "1.0000000000000002e-06\n",
      "Epoch [55/1000], Loss: 1.3437\n",
      "1.0000000000000002e-06\n",
      "Epoch [56/1000], Loss: 1.3434\n",
      "1.0000000000000002e-06\n",
      "Epoch [57/1000], Loss: 1.3432\n",
      "1.0000000000000002e-06\n",
      "Epoch [58/1000], Loss: 1.3429\n",
      "1.0000000000000002e-06\n",
      "Epoch [59/1000], Loss: 1.3427\n",
      "1.0000000000000002e-06\n",
      "Epoch [60/1000], Loss: 1.3425\n",
      "1.0000000000000002e-06\n",
      "Epoch [61/1000], Loss: 1.3423\n",
      "1.0000000000000002e-06\n",
      "Epoch [62/1000], Loss: 1.3421\n",
      "1.0000000000000002e-06\n",
      "Epoch [63/1000], Loss: 1.3419\n",
      "1.0000000000000002e-06\n",
      "Epoch [64/1000], Loss: 1.3417\n",
      "1.0000000000000002e-06\n",
      "Epoch [65/1000], Loss: 1.3415\n",
      "1.0000000000000002e-06\n",
      "Epoch [66/1000], Loss: 1.3413\n",
      "1.0000000000000002e-06\n",
      "Epoch [67/1000], Loss: 1.3412\n",
      "1.0000000000000002e-06\n",
      "Epoch [68/1000], Loss: 1.3410\n",
      "1.0000000000000002e-06\n",
      "Epoch [69/1000], Loss: 1.3408\n",
      "1.0000000000000002e-06\n",
      "Epoch [70/1000], Loss: 1.3406\n",
      "1.0000000000000002e-06\n",
      "Epoch [71/1000], Loss: 1.3404\n",
      "1.0000000000000002e-06\n",
      "Epoch [72/1000], Loss: 1.3402\n",
      "1.0000000000000002e-06\n",
      "Epoch [73/1000], Loss: 1.3400\n",
      "1.0000000000000002e-06\n",
      "Epoch [74/1000], Loss: 1.3398\n",
      "1.0000000000000002e-06\n",
      "Epoch [75/1000], Loss: 1.3395\n",
      "1.0000000000000002e-06\n",
      "Epoch [76/1000], Loss: 1.3393\n",
      "1.0000000000000002e-06\n",
      "Epoch [77/1000], Loss: 1.3391\n",
      "1.0000000000000002e-06\n",
      "Epoch [78/1000], Loss: 1.3389\n",
      "1.0000000000000002e-06\n",
      "Epoch [79/1000], Loss: 1.3387\n",
      "1.0000000000000002e-06\n",
      "Epoch [80/1000], Loss: 1.3384\n",
      "1.0000000000000002e-06\n",
      "Epoch [81/1000], Loss: 1.3382\n",
      "1.0000000000000002e-06\n",
      "Epoch [82/1000], Loss: 1.3380\n",
      "1.0000000000000002e-06\n",
      "Epoch [83/1000], Loss: 1.3377\n",
      "1.0000000000000002e-06\n",
      "Epoch [84/1000], Loss: 1.3375\n",
      "1.0000000000000002e-06\n",
      "Epoch [85/1000], Loss: 1.3373\n",
      "1.0000000000000002e-06\n",
      "Epoch [86/1000], Loss: 1.3371\n",
      "1.0000000000000002e-06\n",
      "Epoch [87/1000], Loss: 1.3368\n",
      "1.0000000000000002e-06\n",
      "Epoch [88/1000], Loss: 1.3366\n",
      "1.0000000000000002e-06\n",
      "Epoch [89/1000], Loss: 1.3363\n",
      "1.0000000000000002e-06\n",
      "Epoch [90/1000], Loss: 1.3361\n",
      "1.0000000000000002e-06\n",
      "Epoch [91/1000], Loss: 1.3358\n",
      "1.0000000000000002e-06\n",
      "Epoch [92/1000], Loss: 1.3355\n",
      "1.0000000000000002e-06\n",
      "Epoch [93/1000], Loss: 1.3353\n",
      "1.0000000000000002e-06\n",
      "Epoch [94/1000], Loss: 1.3350\n",
      "1.0000000000000002e-06\n",
      "Epoch [95/1000], Loss: 1.3347\n",
      "1.0000000000000002e-06\n",
      "Epoch [96/1000], Loss: 1.3344\n",
      "1.0000000000000002e-06\n",
      "Epoch [97/1000], Loss: 1.3341\n",
      "1.0000000000000002e-06\n",
      "Epoch [98/1000], Loss: 1.3338\n",
      "1.0000000000000002e-06\n",
      "Epoch [99/1000], Loss: 1.3335\n",
      "1.0000000000000002e-06\n",
      "Epoch [100/1000], Loss: 1.3332\n",
      "1.0000000000000002e-06\n",
      "Epoch [101/1000], Loss: 1.3329\n",
      "1.0000000000000002e-06\n",
      "Epoch [102/1000], Loss: 1.3326\n",
      "1.0000000000000002e-06\n",
      "Epoch [103/1000], Loss: 1.3323\n",
      "1.0000000000000002e-06\n",
      "Epoch [104/1000], Loss: 1.3319\n",
      "1.0000000000000002e-06\n",
      "Epoch [105/1000], Loss: 1.3316\n",
      "1.0000000000000002e-06\n",
      "Epoch [106/1000], Loss: 1.3313\n",
      "1.0000000000000002e-06\n",
      "Epoch [107/1000], Loss: 1.3310\n",
      "1.0000000000000002e-06\n",
      "Epoch [108/1000], Loss: 1.3307\n",
      "1.0000000000000002e-06\n",
      "Epoch [109/1000], Loss: 1.3303\n",
      "1.0000000000000002e-06\n",
      "Epoch [110/1000], Loss: 1.3300\n",
      "1.0000000000000002e-06\n",
      "Epoch [111/1000], Loss: 1.3297\n",
      "1.0000000000000002e-06\n",
      "Epoch [112/1000], Loss: 1.3293\n",
      "1.0000000000000002e-06\n",
      "Epoch [113/1000], Loss: 1.3290\n",
      "1.0000000000000002e-06\n",
      "Epoch [114/1000], Loss: 1.3287\n",
      "1.0000000000000002e-06\n",
      "Epoch [115/1000], Loss: 1.3283\n",
      "1.0000000000000002e-06\n",
      "Epoch [116/1000], Loss: 1.3280\n",
      "1.0000000000000002e-06\n",
      "Epoch [117/1000], Loss: 1.3277\n",
      "1.0000000000000002e-06\n",
      "Epoch [118/1000], Loss: 1.3273\n",
      "1.0000000000000002e-06\n",
      "Epoch [119/1000], Loss: 1.3270\n",
      "1.0000000000000002e-06\n",
      "Epoch [120/1000], Loss: 1.3267\n",
      "1.0000000000000002e-06\n",
      "Epoch [121/1000], Loss: 1.3263\n",
      "1.0000000000000002e-06\n",
      "Epoch [122/1000], Loss: 1.3260\n",
      "1.0000000000000002e-06\n",
      "Epoch [123/1000], Loss: 1.3257\n",
      "1.0000000000000002e-06\n",
      "Epoch [124/1000], Loss: 1.3253\n",
      "1.0000000000000002e-06\n",
      "Epoch [125/1000], Loss: 1.3250\n",
      "1.0000000000000002e-06\n",
      "Epoch [126/1000], Loss: 1.3246\n",
      "1.0000000000000002e-06\n",
      "Epoch [127/1000], Loss: 1.3243\n",
      "1.0000000000000002e-06\n",
      "Epoch [128/1000], Loss: 1.3239\n",
      "1.0000000000000002e-06\n",
      "Epoch [129/1000], Loss: 1.3236\n",
      "1.0000000000000002e-06\n",
      "Epoch [130/1000], Loss: 1.3232\n",
      "1.0000000000000002e-06\n",
      "Epoch [131/1000], Loss: 1.3228\n",
      "1.0000000000000002e-06\n",
      "Epoch [132/1000], Loss: 1.3225\n",
      "1.0000000000000002e-06\n",
      "Epoch [133/1000], Loss: 1.3221\n",
      "1.0000000000000002e-06\n",
      "Epoch [134/1000], Loss: 1.3217\n",
      "1.0000000000000002e-06\n",
      "Epoch [135/1000], Loss: 1.3214\n",
      "1.0000000000000002e-06\n",
      "Epoch [136/1000], Loss: 1.3210\n",
      "1.0000000000000002e-06\n",
      "Epoch [137/1000], Loss: 1.3206\n",
      "1.0000000000000002e-06\n",
      "Epoch [138/1000], Loss: 1.3203\n",
      "1.0000000000000002e-06\n",
      "Epoch [139/1000], Loss: 1.3199\n",
      "1.0000000000000002e-06\n",
      "Epoch [140/1000], Loss: 1.3196\n",
      "1.0000000000000002e-06\n",
      "Epoch [141/1000], Loss: 1.3192\n",
      "1.0000000000000002e-06\n",
      "Epoch [142/1000], Loss: 1.3188\n",
      "1.0000000000000002e-06\n",
      "Epoch [143/1000], Loss: 1.3185\n",
      "1.0000000000000002e-06\n",
      "Epoch [144/1000], Loss: 1.3181\n",
      "1.0000000000000002e-06\n",
      "Epoch [145/1000], Loss: 1.3178\n",
      "1.0000000000000002e-06\n",
      "Epoch [146/1000], Loss: 1.3174\n",
      "1.0000000000000002e-06\n",
      "Epoch [147/1000], Loss: 1.3171\n",
      "1.0000000000000002e-06\n",
      "Epoch [148/1000], Loss: 1.3167\n",
      "1.0000000000000002e-06\n",
      "Epoch [149/1000], Loss: 1.3164\n",
      "1.0000000000000002e-06\n",
      "Epoch [150/1000], Loss: 1.3160\n",
      "1.0000000000000002e-06\n",
      "Epoch [151/1000], Loss: 1.3156\n",
      "1.0000000000000002e-06\n",
      "Epoch [152/1000], Loss: 1.3153\n",
      "1.0000000000000002e-06\n",
      "Epoch [153/1000], Loss: 1.3149\n",
      "1.0000000000000002e-06\n",
      "Epoch [154/1000], Loss: 1.3145\n",
      "1.0000000000000002e-06\n",
      "Epoch [155/1000], Loss: 1.3142\n",
      "1.0000000000000002e-06\n",
      "Epoch [156/1000], Loss: 1.3138\n",
      "1.0000000000000002e-06\n",
      "Epoch [157/1000], Loss: 1.3134\n",
      "1.0000000000000002e-06\n",
      "Epoch [158/1000], Loss: 1.3130\n",
      "1.0000000000000002e-06\n",
      "Epoch [159/1000], Loss: 1.3127\n",
      "1.0000000000000002e-06\n",
      "Epoch [160/1000], Loss: 1.3123\n",
      "1.0000000000000002e-06\n",
      "Epoch [161/1000], Loss: 1.3119\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model, scheduler, criterion, EPOCHS, train_data)\n\u001b[1;32m      2\u001b[0m \u001b[39m#train(model, optimizer, criterion, EPOCHS, train_data)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[94], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, scheduler, criterion, num_epochs, train_loader)\u001b[0m\n\u001b[1;32m     12\u001b[0m     scheduler\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 14\u001b[0m     scheduler\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     15\u001b[0m scheduler\u001b[39m.\u001b[39mstep(loss)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(scheduler\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    235\u001b[0m          grads,\n\u001b[1;32m    236\u001b[0m          exp_avgs,\n\u001b[1;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    239\u001b[0m          state_steps,\n\u001b[1;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m func(params,\n\u001b[1;32m    301\u001b[0m      grads,\n\u001b[1;32m    302\u001b[0m      exp_avgs,\n\u001b[1;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    305\u001b[0m      state_steps,\n\u001b[1;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    408\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[1;32m    412\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, scheduler, criterion, EPOCHS, train_data)\n",
    "#train(model, optimizer, criterion, EPOCHS, train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = torch.FloatTensor(X_test)\n",
    "test_loader = DataLoader(dataset=Xtest, batch_size=1)\n",
    "\n",
    "# Set the model to eval mode and generate a list of predictions for the test data\n",
    "model.eval()\n",
    "y_pred_list = []\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_pred_list.append(y_test_pred.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "y_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of only the first object of each array inside y_pred_list\n",
    "transaction_count_predictions = [a[0] for a in y_pred_list]\n",
    "transaction_count_actuals = [a[0] for a in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the MSE and MAE between the predictions and the actual values\n",
    "mse = mean_squared_error(y_test, y_pred_list)\n",
    "mae = mean_absolute_error(y_test, y_pred_list)\n",
    "\n",
    "print(f'MSE: {mse:.2f}, MAE: {mae:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the MSE and MAE between only the transaction_count prediction and the actual values\n",
    "mse = mean_squared_error(transaction_count_actuals, transaction_count_predictions)\n",
    "mae = mean_absolute_error(transaction_count_actuals, transaction_count_predictions)\n",
    "\n",
    "print(f'MSE: {mse:.2f}, MAE: {mae:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SMAPE between actual and predicted values.\n",
    "def smape(actual, predicted):\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "    smape_val = (100.0 / actual.size) * np.sum(2.0 * np.abs(predicted - actual) / (np.abs(actual) + np.abs(predicted)))\n",
    "    return smape_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape = smape(y_pred_list, y_test)\n",
    "#print(f'SMAPE: {smape:.2f}')\n",
    "print(smape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the predictions vs the actual values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.scatter(transaction_count_predictions, transaction_count_actuals, alpha=0.5)\n",
    "plt.plot(transaction_count_actuals[400:600], label='Actual')\n",
    "plt.plot(transaction_count_predictions[400:600], label='Predicted')\n",
    "plt.xlabel('Data Point Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Predictions vs Actual Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
